# BERT_Notes
BERTå­¦ä¹ ç¬”è®°+æ–‡æœ¬åˆ†ç±»å®ç°

## BERT
BERTæ˜¯ä¸€ä¸ªåŸºäºTransformerçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œåˆ©ç”¨äº†Transformerçš„ç¼–ç å™¨è¿›è¡Œå åŠ ï¼Œä»è€Œåœ¨è®¸å¤šNLPä»»åŠ¡ä¸Šå–å¾—äº†sotaçš„æ•ˆæœã€‚  
![å›¾ç‰‡](https://user-images.githubusercontent.com/126166790/224623896-681be04c-ea03-46c9-b6df-89bf268b6809.png)  


å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨äº†åŒå‘æ€ç»´ï¼Œè®ºæ–‡ä¸­è¯´åˆ°è¯­è¨€æ¨¡å‹çš„ä¸»è¦é™åˆ¶æ˜¯å•å‘æ€§ï¼Œè€Œè¿™é™åˆ¶äº†é¢„è®­ç»ƒæ—¶å¯é€‰æ‹©çš„æ¶æ„ã€‚æ–‡ä¸­æåˆ°äº†BERTæ¯”ç®€å•çš„å•ä¸€æ–¹å‘ä»¥åŠä»å·¦åˆ°å³ç»“åˆä»å³åˆ°å·¦ï¼ˆconcatenationï¼‰æ•ˆæœéƒ½è¦å¥½ï¼Œå¹¶ä¸”åˆ©ç”¨äº†ä¸¤ç§é¢„è®­ç»ƒçš„ä»»åŠ¡ï¼ˆå®Œå½¢å¡«ç©ºå’Œä¸‹å¥åˆ¤æ–­ï¼‰ï¼Œè¿™æ ·å¾—åˆ°çš„é¢„è®­ç»ƒæ¨¡å‹åªéœ€è¦å¾®è°ƒï¼ˆfine-tuningï¼‰å°±èƒ½åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å¾—åˆ°ä¸é”™çš„æ•ˆæœã€‚

## é¢„è®­ç»ƒä»»åŠ¡1â€”â€”å®Œå½¢å¡«ç©ºï¼ˆMasked LMï¼‰
ä½œè€…å°†æ–‡æœ¬éšæœºçš„15%çš„å†…å®¹ç”¨[MASK]è¿›è¡Œå¤„ç†ï¼Œæœ‰80%çš„å‡ ç‡è¢«çœŸå®åœ°æ›¿æ¢ä¸º[MASK],æœ‰10%çš„å‡ ç‡éšæœºæ›¿æ¢ä¸ºå…¶ä»–è¯ï¼Œè¿˜æœ‰10%ä¸æ”¹å˜ï¼Œè¿™æ˜¯å› ä¸ºè€ƒè™‘åˆ°å®é™…çš„ä¸‹æ¸¸ä»»åŠ¡ä¸­å¹¶æ²¡æœ‰å‡ºç°[MASK].

## é¢„è®­ç»ƒä»»åŠ¡2â€”â€”ä¸‹å¥åˆ¤æ–­ï¼ˆNext Sentence Prediction NSPï¼‰
å¯ä»¥è¾“å…¥1ä¸ªå¥å­ä¹Ÿå¯ä»¥è¾“å…¥2ä¸ªå¥å­ï¼Œç”¨æ¥é¢„æµ‹ç¬¬äºŒä¸ªå¥å­æ˜¯ä¸æ˜¯ä½äºç¬¬ä¸€ä¸ªå¥å­ä¹‹åï¼Œç”¨3ä¸ªembeddingå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œè¡¨ç¤ºã€‚

æ–‡ç« çš„ååŠéƒ¨åˆ†ä¸»è¦æ˜¯å…³äºåœ¨å„ä¸ªä»»åŠ¡ä¸Šçš„å®éªŒè¡¨ç°ä»¥åŠæ¶ˆèå®éªŒï¼Œåœ¨é™„å½•ä¸­è¯¦ç»†ä»‹ç»äº†é¢„è®­ç»ƒå’Œå¾®è°ƒæµç¨‹ã€‚  

## åˆ©ç”¨BERTåšä¸€ä¸ªæ–‡æœ¬åˆ†ç±»ä»»åŠ¡
ä½¿ç”¨pytorchå’Œhugging faceçš„transformersåº“ï¼Œæ•°æ®é›†ä¸ºsentiment-analysis-on-movie-reviewsã€‚  
>ğŸ¤—æŠ±æŠ±è„¸ï¼š<https://github.com/huggingface/transformers>  

æ•°æ®é›†ä»‹ç»:å¤§è‡´é•¿è¿™æ ·ï¼Œåœ¨æ¯ä¸ªå¥å­åé¢çš„æ•°å­—æ˜¯æƒ…æ„Ÿã€‚  
æƒ…æ„Ÿæœ‰5ç§ï¼š**0.negative  1.somewhat negative  2.neutral  3.somewhat positive  4.positive**
![å›¾ç‰‡](https://user-images.githubusercontent.com/126166790/224625403-2fc4eee1-f6c2-4408-b97c-d884db4f6c7b.png)



### ä¸»è¦æ­¥éª¤ä¸ºï¼š
### 1ã€æ–‡æœ¬åµŒå…¥è¡¨ç¤º  
å…³äºæ–‡æœ¬çš„åµŒå…¥ï¼Œä½¿ç”¨çš„æ˜¯transformersåº“æä¾›çš„AutoTokenizerï¼Œéå¸¸æ–¹ä¾¿ã€‚  
'''python    
def text2embedding(text):
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    embeding = tokenizer(text, padding="max_length", truncation=True, max_length=MAX_LENGTH)  # è¶…å‡ºæœ€å¤§é•¿åº¦æ—¶æˆªæ–­
    return embeding

'''
### 2ã€å°†5ç§æƒ…æ„Ÿè½¬ä¸ºç‹¬çƒ­ç è¡¨ç¤º  
### 3ã€æ­å»ºåŸºäºBERTçš„æ¨¡å‹  
### 4ã€è®­ç»ƒ
åœ¨åˆå§‹æ•°æ®å¤„ç†é˜¶æ®µè¿˜å¯¹æ•°æ®é›†è¿›è¡Œäº†åˆ’åˆ†ï¼Œå…ˆæ€»ä½“æ‰“ä¹±ï¼Œç„¶åå°†80%ä½œä¸ºè®­ç»ƒé›†ï¼Œå‰©ä¸‹20%ä½œä¸ºæµ‹è¯•é›†ã€‚å…³äºæ–‡æœ¬çš„åµŒå…¥ï¼Œä½¿ç”¨çš„æ˜¯transformersåº“æä¾›çš„AutoTokenizerï¼Œéå¸¸æ–¹ä¾¿ã€‚

ä½¿ç”¨ç‹¬çƒ­ç¼–ç å³å¯ã€‚

æ¨¡å‹æ„å»ºï¼šæ¯”è¾ƒç®€å•ï¼ŒåŠ è½½BERTé¢„è®­ç»ƒæ¨¡å‹ï¼Œç„¶ååŠ äº†ä¸€ä¸ªdropoutï¼Œä»¥åŠä¸€ä¸ªLinearå±‚ã€‚
æŸå¤±å‡½æ•°ä½¿ç”¨äº¤å‰ç†µï¼Œä½†æ˜¯é€‚åˆæ•ˆæœæ²¡å¤ªå¥½ï¼Œä¼˜åŒ–å™¨ç”¨Adamã€‚å®Œæ•´ä»£ç å¦‚ä¸‹ã€‚
